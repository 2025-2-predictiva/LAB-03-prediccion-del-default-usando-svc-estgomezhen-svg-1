{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c6b7e287",
   "metadata": {},
   "source": [
    "# Predicción de Default de Clientes con Regresión Logística\n",
    "\n",
    "En este cuaderno desarrolla un modelo de clasificación para predecir si un cliente\n",
    "incurrirá en default el próximo mes. El dataset incluye 23 variables relacionadas\n",
    "con historial crediticio, pagos y características demográficas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7df56240",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All modules imported successfully.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.metrics import balanced_accuracy_score, make_scorer\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "from sklearn.svm import SVC\n",
    "import gzip\n",
    "import json\n",
    "import os\n",
    "import pickle\n",
    "from sklearn.metrics import precision_score, balanced_accuracy_score, recall_score, f1_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "print(\"All modules imported successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edeeb8fd",
   "metadata": {},
   "source": [
    "# Paso 1.\n",
    "- Realice la limpieza de los datasets:\n",
    "- Renombre la columna \"default payment next month\" a \"default\".\n",
    "- Remueva la columna \"ID\".\n",
    "- Elimine los registros con informacion no disponible.\n",
    "- Para la columna EDUCATION, valores > 4 indican niveles superiores de educación, agrupe estos valores en la categoría \"others\".\n",
    "- Renombre la columna \"default payment next month\" a \"default\"\n",
    "- Remueva la columna \"ID\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4eef9e9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datos cargados correctamente.\n"
     ]
    }
   ],
   "source": [
    "#carga de datos\n",
    "train = pd.read_csv(\n",
    "    \"../files/input/train_data.csv.zip\",\n",
    "    compression=\"zip\",\n",
    ")\n",
    "test = pd.read_csv(\n",
    "    \"../files/input/test_data.csv.zip\",\n",
    "    compression=\"zip\",\n",
    ")\n",
    "print(\"Datos cargados correctamente.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f19d50f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datos preprocesados correctamente.\n"
     ]
    }
   ],
   "source": [
    "# Renombrar variable objetivo\n",
    "train = train.rename(columns={\"default payment next month\": \"default\"})\n",
    "test = test.rename(columns={\"default payment next month\": \"default\"})\n",
    "\n",
    "# Eliminar columna ID \n",
    "if \"ID\" in train.columns:\n",
    "    train = train.drop(columns=[\"ID\"])\n",
    "if \"ID\" in test.columns:\n",
    "    test = test.drop(columns=[\"ID\"])\n",
    "\n",
    "# Eliminar filas con valores faltantes\n",
    "train = train.dropna()\n",
    "test = test.dropna()\n",
    "\n",
    "# EDUCATION > 4 se agrupa en categoría \"otros\" (4)\n",
    "train[\"EDUCATION\"] = train[\"EDUCATION\"].clip(upper=4)\n",
    "test[\"EDUCATION\"] = test[\"EDUCATION\"].clip(upper=4)\n",
    "\n",
    "#Eliminación de los registro de información no disponible\n",
    "train = train.loc[train[\"EDUCATION\"] != 0]\n",
    "train = train.loc[train[\"MARRIAGE\"] != 0]\n",
    "\n",
    "test = test.loc[test[\"EDUCATION\"] != 0]\n",
    "test = test.loc[test[\"MARRIAGE\"] != 0]\n",
    "\n",
    "print(\"Datos preprocesados correctamente.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28abb6c7",
   "metadata": {},
   "source": [
    "# Paso 2.\n",
    "- Divida los datasets en x_train, y_train, x_test, y_test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d2d766a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variables independientes y dependientes separadas.\n"
     ]
    }
   ],
   "source": [
    "x_train = train.drop(columns=\"default\")\n",
    "y_train = train[\"default\"]\n",
    "\n",
    "x_test = test.drop(columns=\"default\")\n",
    "y_test = test[\"default\"]\n",
    "\n",
    "print(\"Variables independientes y dependientes separadas.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72d5927a",
   "metadata": {},
   "source": [
    "# Paso 3.\n",
    "Cree un pipeline para el modelo de clasificación. Este pipeline debe contener las siguientes capas:\n",
    "\n",
    " - Transforma las variables categoricas usando el método one-hot-encoding.\n",
    " - Descompone la matriz de entrada usando PCA. El PCA usa todas las componentes.\n",
    " - Estandariza la matriz de entrada.\n",
    " - Selecciona las K columnas mas relevantes de la matrix de entrada.\n",
    " - Ajusta una maquina de vectores de soporte (svm).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "108fdf06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocesador creado correctamente.\n"
     ]
    }
   ],
   "source": [
    "#Columnas categóricas \n",
    "categorical_features=[\"SEX\", \"EDUCATION\", \"MARRIAGE\"]\n",
    "numerical_features = num_columns = [col for col in x_train.columns if col not in categorical_features]\n",
    "\n",
    "#Preprocesador\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('cat', OneHotEncoder(), categorical_features),\n",
    "        ('scaler', StandardScaler(with_mean=True, with_std=True), numerical_features),\n",
    "    ],\n",
    ")\n",
    "\n",
    "print(\"Preprocesador creado correctamente.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2190f852",
   "metadata": {},
   "source": [
    "# Paso 4.\n",
    "- Optimice los hiperparametros del pipeline usando validación cruzada.\n",
    "- Use 10 splits para la validación cruzada. Use la función de precision\n",
    "- balanceada para medir la precisión del modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d7962f64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline creado.\n",
      "Pipeline(steps=[('preprocessor',\n",
      "                 ColumnTransformer(transformers=[('cat', OneHotEncoder(),\n",
      "                                                  ['SEX', 'EDUCATION',\n",
      "                                                   'MARRIAGE']),\n",
      "                                                 ('scaler', StandardScaler(),\n",
      "                                                  ['LIMIT_BAL', 'AGE', 'PAY_0',\n",
      "                                                   'PAY_2', 'PAY_3', 'PAY_4',\n",
      "                                                   'PAY_5', 'PAY_6',\n",
      "                                                   'BILL_AMT1', 'BILL_AMT2',\n",
      "                                                   'BILL_AMT3', 'BILL_AMT4',\n",
      "                                                   'BILL_AMT5', 'BILL_AMT6',\n",
      "                                                   'PAY_AMT1', 'PAY_AMT2',\n",
      "                                                   'PAY_AMT3', 'PAY_AMT4',\n",
      "                                                   'PAY_AMT5', 'PAY_AMT6'])])),\n",
      "                ('pca', PCA()), ('feature_selection', SelectKBest()),\n",
      "                ('classifier', SVC(random_state=12345))])\n"
     ]
    }
   ],
   "source": [
    "#Creamos el pipeline\n",
    "pipeline = Pipeline(\n",
    "    steps=[\n",
    "        (\"preprocessor\", preprocessor),\n",
    "        ('pca', PCA()),\n",
    "        ('feature_selection', SelectKBest(score_func=f_classif)),\n",
    "        (\"classifier\", SVC(kernel=\"rbf\", random_state=12345, max_iter=-1)),\n",
    "    ],\n",
    ")\n",
    "print(\"Pipeline creado.\")\n",
    "print(pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "84d2434b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modelo entrenado correctamente.\n",
      "Mejores hiperparámetros encontrados:\n"
     ]
    }
   ],
   "source": [
    "param_grid = {\n",
    "    'pca__n_components':[20, x_train.shape[1]-2],\n",
    "    'feature_selection__k':[12],\n",
    "    'classifier__kernel': ['rbf'],\n",
    "    'classifier__gamma': [0.1],\n",
    "}\n",
    "\n",
    "model=GridSearchCV(\n",
    "    pipeline,\n",
    "    param_grid,\n",
    "    cv=10,\n",
    "    scoring=\"balanced_accuracy\",\n",
    "    n_jobs=-1,\n",
    "    refit=True\n",
    "    )\n",
    "\n",
    "model.fit(x_train, y_train)\n",
    "print(\"Modelo entrenado correctamente.\")\n",
    "print(\"Mejores hiperparámetros encontrados:\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc5d7d0e",
   "metadata": {},
   "source": [
    "# Paso 5.\n",
    "- Guarde el modelo (comprimido con gzip) como \"files/models/model.pkl.gz\".\n",
    "- Recuerde que es posible guardar el modelo comprimido usanzo la libreria gzip."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d4728e43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modelo guardado en ../files/models/model.pkl.gz\n"
     ]
    }
   ],
   "source": [
    "models_dir = '../files/models'\n",
    "os.makedirs(models_dir, exist_ok=True)\n",
    "\n",
    "#Nombre del archivo\n",
    "compressed_model_path = \"../files/models/model.pkl.gz\"\n",
    "\n",
    "with gzip.open(compressed_model_path, \"wb\") as file:\n",
    "    pickle.dump(model, file)\n",
    "print(f\"Modelo guardado en {compressed_model_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b56f692",
   "metadata": {},
   "source": [
    "# Paso 6.\n",
    "- Calcule las metricas de precision, precision balanceada, recall, y f1-score para los conjuntos de entrenamiento y prueba.\n",
    "- Guardelas en el archivo files/output/metrics.json. Cada fila del archivo es un diccionario con las metricas de un modelo.\n",
    "- Este diccionario tiene un campo para indicar si es el conjunto de entrenamiento o prueba. Por ejemplo:\n",
    "- - {'dataset': 'train', 'precision': 0.8, 'balanced_accuracy': 0.7, 'recall': 0.9, 'f1_score': 0.85}\n",
    "- - {'dataset': 'test', 'precision': 0.7, 'balanced_accuracy': 0.6, 'recall': 0.8, 'f1_score': 0.75}\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "db8f8a06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Métricas calculadas y guardadas correctamente.\n"
     ]
    }
   ],
   "source": [
    "def calculate_and_save_metrics(model, X_train, X_test, y_train, y_test):\n",
    "    #Prediciones\n",
    "    y_train_pred = model.predict(X_train)\n",
    "    y_test_pred = model.predict(X_test)\n",
    "\n",
    "    #Metricas para el entrenamiento\n",
    "    metrics_train = {\n",
    "        'type': 'metrics',\n",
    "        'dataset': 'train',\n",
    "        'precision': precision_score(y_train, y_train_pred, zero_division=0),\n",
    "        'balanced_accuracy': balanced_accuracy_score(y_train, y_train_pred),\n",
    "        'recall': recall_score(y_train, y_train_pred, zero_division=0),\n",
    "        'f1_score': f1_score(y_train, y_train_pred, zero_division=0)\n",
    "    }\n",
    "\n",
    "    #Metricas para las pruebas\n",
    "    metrics_test = {\n",
    "        'type': 'metrics',\n",
    "        'dataset': 'test',\n",
    "        'precision' : precision_score(y_test, y_test_pred, zero_division=0),\n",
    "        'balanced_accuracy' : balanced_accuracy_score(y_test, y_test_pred),\n",
    "        'recall' : recall_score(y_test, y_test_pred, zero_division=0),\n",
    "        'f1_score' : f1_score(y_test, y_test_pred, zero_division=0)\n",
    "    }\n",
    "\n",
    "    #Crear carpeta si no existe\n",
    "    output_dir = '../files/output'\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    #Guardar las metricas en un archivo JSON\n",
    "    output_path = os.path.join(output_dir, 'metrics.json')\n",
    "    with open(output_path, 'w') as f: #w para comenzar con un archivo limpio\n",
    "        f.write(json.dumps(metrics_train) + '\\n')\n",
    "        f.write(json.dumps(metrics_test) + '\\n')\n",
    "print(\"Métricas calculadas y guardadas correctamente.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4834870",
   "metadata": {},
   "source": [
    "# Paso 7.\n",
    "- Calcule las matrices de confusion para los conjuntos de entrenamiento y prueba. \n",
    "- Guardelas en el archivo files/output/metrics.json. Cada fila del archivo es un diccionario con las metricas de un modelo. de entrenamiento o prueba. Por ejemplo:\n",
    "- - {'type': 'cm_matrix', 'dataset': 'train', 'true_0': {\"predicted_0\": 15562, \"predicte_1\": 666}, 'true_1': {\"predicted_0\": 3333, \"predicted_1\": 1444}}\n",
    "- -{'type': 'cm_matrix', 'dataset': 'test', 'true_0': {\"predicted_0\": 15562, \"predicte_1\": 650}, 'true_1': {\"predicted_0\": 2490, \"predicted_1\": 1420}}\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ff75c038",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Métricas y matrices de confusión calculadas y guardadas correctamente.\n"
     ]
    }
   ],
   "source": [
    "def calculate_and_save_confusion_matrices(model, X_train, X_test, y_train, y_test):\n",
    "    #Predicciones\n",
    "    y_train_pred = model.predict(X_train)\n",
    "    y_test_pred = model.predict(X_test)\n",
    "\n",
    "    #Calcular matrices de confusion\n",
    "    cm_train = confusion_matrix(y_train, y_train_pred)\n",
    "    cm_test = confusion_matrix(y_test, y_test_pred)\n",
    "\n",
    "    #Convertir las matrices de confision en formato json\n",
    "    def format_confussion_matrix(cm, dataset_type):\n",
    "        return {\n",
    "            'type': 'cm_matrix', \n",
    "            'dataset': dataset_type,\n",
    "            'true_0': {\n",
    "                'predicted_0': int(cm_train[0, 0]), \n",
    "                'predicted_1': int(cm_train[0, 1])\n",
    "            },\n",
    "            'true_1': {\n",
    "                'predicted_0': int(cm_train[1, 0]), \n",
    "                'predicted_1': int(cm_train[1, 1])\n",
    "            },\n",
    "        }\n",
    "\n",
    "    metrics = [\n",
    "        format_confussion_matrix(cm_train, 'train'),\n",
    "        format_confussion_matrix(cm_test, 'test')\n",
    "    ]\n",
    "\n",
    "    #Guardar las matrices de confusión en el mismo archivo json\n",
    "    output_path = '../files/output/metrics.json'\n",
    "    with open(output_path, 'a') as f: #'a' para agregar después de las metricas\n",
    "        for metric in metrics:\n",
    "            f.write(json.dumps(metric) + '\\n')\n",
    "\n",
    "#Función principal para ejecutar todo\n",
    "def main(model, X_train, X_test, y_train, y_test):\n",
    "    #Crear el directorio de salida si no existe\n",
    "    os.makedirs('../files/output', exist_ok=True)\n",
    "\n",
    "    #Calcular y guardar las metricas\n",
    "    calculate_and_save_metrics(model, X_train, X_test, y_train, y_test)\n",
    "\n",
    "    #Calcular y guardar las matrices de confusión\n",
    "    calculate_and_save_confusion_matrices(model, X_train, X_test, y_train, y_test)\n",
    "\n",
    "\n",
    "print(\"Métricas y matrices de confusión calculadas y guardadas correctamente.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
